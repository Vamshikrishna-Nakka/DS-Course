{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22fc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.SparkSession ------ Main entry point for DataFrame and SQL functionality.\n",
    "# pyspark.sql.DataFrame ------ A distributed collection of data grouped into named columns.\n",
    "# pyspark.sql.Column ----------A column expression in a DataFrame.\n",
    "# pyspark.sql.Row ---------- A row of data in a DataFrame.\n",
    "# pyspark.sql.GroupedData ------- Aggregation methods, returned by DataFrame.groupBy().\n",
    "# pyspark.sql.DataFrameNaFunctions ----- Methods for handling missing data (null values).\n",
    "# pyspark.sql.DataFrameStatFunctions ------ Methods for statistics functionality.\n",
    "# pyspark.sql.functions --- List of built-in functions available for DataFrame.\n",
    "# pyspark.sql.types --- List of data types available.\n",
    "# pyspark.sql.Window --- For working with window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9319be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefdf96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkSession.builder.appName(\"PysparkExample\")\\    \n",
    "# .config (\"spark.sql.shuffle.partitions\", \"50\")\\    .config(\"spark.driver.maxResultSize\",\"5g\")\\    \n",
    "# .config (\"spark.sql.execution.arrow.enabled\", \"true\")\\    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21bc92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Testing\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aba7630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LT387.CORP.SYSTECHMNG.NET:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17b70ccc250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323a4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----creating RDD -----\n",
    "\n",
    "# columns = [\"language\",\"users_count\"]\n",
    "# a = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "# r = spark.sparkContext.parallelize(a)  #creating an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b338c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Using toDF() function creating Dataframe------------\n",
    "\n",
    "# rd = r.toDF(columns)\n",
    "# rd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff12e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56315a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0147ccf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----Using createDataFrame() from SparkSession creating Dataframe-----\n",
    "\n",
    "# c = spark.createDataFrame(r).toDF(*columns)\n",
    "# or\n",
    "# c = spark.createDataFrame(data = a , schema = columns)\n",
    "# df3 = spark.createDataFrame([], StructType([])) #empty df\n",
    "\n",
    "# print(c.printSchema())\n",
    "# print(type(c))\n",
    "# print(c.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d985c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Create DataFrame with schema----------------------\n",
    "# from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "# data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "#     (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "#     (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "#     (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "#     (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "#   ]\n",
    "\n",
    "# schema = StructType([ \\\n",
    "#     StructField(\"firstname\",StringType(),True), \\\n",
    "#     StructField(\"middlename\",StringType(),True), \\\n",
    "#     StructField(\"lastname\",StringType(),True), \\\n",
    "#     StructField(\"id\", StringType(), True), \\\n",
    "#     StructField(\"gender\", StringType(), True), \\\n",
    "#     StructField(\"salary\", IntegerType(), True) \\\n",
    "#   ])\n",
    " \n",
    "# df = spark.createDataFrame(data=data2,schema=schema)\n",
    "# df.printSchema()\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81ccba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------Create DataFrame from Data sources---------------\n",
    "# schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n",
    "# df = spark.read.option('header','true').csv('heart.csv')\n",
    "# df = spark.read.csv('/Users/mreznik/heart.csv',inferSchema=True, schema=schema,nullValue='NA' header=True)\n",
    "# df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "# df = spark.read.load('parquet_data.parquet')\n",
    "# df = spark.read.load(\"Case.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "# df = spark.read.text(\"/src/resources/file.txt\")\n",
    "\n",
    "# df.write.format(\"csv\").save(\"heart_save.csv\")  # save data\n",
    "# df.write.format(\"csv\").mode(\"overwrite\").save(\"heart_save.csv\")  # if you want to overwrite the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "015becc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('credit_record.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "930e2b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- INCOME: string (nullable = true)\n",
      " |-- FAMILY_STATUS: string (nullable = true)\n",
      " |-- HOUSING_TYPE: string (nullable = true)\n",
      " |-- FAM_MEMBERS: string (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7a2031d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+--------------+------------+-----------+------+\n",
      "|     ID|GENDER|INCOME| FAMILY_STATUS|HOUSING_TYPE|FAM_MEMBERS|STATUS|\n",
      "+-------+------+------+--------------+------------+-----------+------+\n",
      "|5008804|     M|427500|Civil marriage|        Rent|          2|     X|\n",
      "|5008805|     M|427500|Civil marriage|        Rent|          2|     0|\n",
      "|5008806|     M|112500|       Married|       House|          2|     0|\n",
      "|5008808|     F|270000|        Single|       House|          1|     0|\n",
      "|5008809|     F|270000|        Single|       House|          1|     C|\n",
      "|5008810|     F|270000|        Single|       House|          1|     C|\n",
      "|5008811|     F|270000|        Single|       House|          1|     C|\n",
      "|5008812|     F|283500|     Separated|       House|          1|     C|\n",
      "|5008813|     F|283500|     Separated|       House|          1|     C|\n",
      "|5008814|     F|283500|     Separated|       House|          1|     C|\n",
      "|5008815|     M|270000|       Married|       House|          2|     C|\n",
      "|5112956|     M|270000|       Married|       House|          2|     C|\n",
      "|6153651|     M|270000|       Married|       House|          2|     C|\n",
      "|5008819|     M|135000|       Married|       House|          2|     0|\n",
      "|5008820|     M|135000|       Married|       House|          2|     0|\n",
      "|5008821|     M|135000|       Married|       House|          2|     0|\n",
      "|5008822|     M|135000|       Married|       House|          2|     0|\n",
      "|5008823|     M|135000|       Married|       House|          2|     0|\n",
      "|5008824|     M|135000|       Married|       House|          2|     0|\n",
      "|5008825|     F|130500|       Married|       House|          2|     0|\n",
      "+-------+------+------+--------------+------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() #\n",
    "# df.show(2,truncate=False) Display 2 rows and full column contents with truncate false\n",
    "# df.show(2,truncate=25) # Display 2 rows & column values 25 characters\n",
    "# df.show(n=3,truncate=25,vertical=True) # Display DataFrame rows & columns vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d314b3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cec6acf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'GENDER',\n",
       " 'INCOME',\n",
       " 'FAMILY_STATUS',\n",
       " 'HOUSING_TYPE',\n",
       " 'FAM_MEMBERS',\n",
       " 'STATUS']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "38d462d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('GENDER', 'string'),\n",
       " ('INCOME', 'string'),\n",
       " ('FAMILY_STATUS', 'string'),\n",
       " ('HOUSING_TYPE', 'string'),\n",
       " ('FAM_MEMBERS', 'string'),\n",
       " ('STATUS', 'string')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9a188a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10047224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+------------------+--------------+---------------+------------------+--------------------+\n",
      "|summary|               ID|GENDER|            INCOME| FAMILY_STATUS|   HOUSING_TYPE|       FAM_MEMBERS|              STATUS|\n",
      "+-------+-----------------+------+------------------+--------------+---------------+------------------+--------------------+\n",
      "|  count|             5000|  5000|              5000|          5000|           5000|              5000|                4999|\n",
      "|   mean|     5140650.5812|  null|        195431.769|          null|           null|            2.1748|0.025404157043879907|\n",
      "| stddev|374123.9451344954|  null|114032.49657124284|          null|           null|0.8525199810348216|  0.1573946918353626|\n",
      "|    min|          5008804|     F|            101250|Civil marriage|Co-op apartment|                 1|                   0|\n",
      "|    max|          6790437|     M|             99900|         Widow|   With parents|                 6|                   X|\n",
      "+-------+-----------------+------+------------------+--------------+---------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2936756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ID='5008804', GENDER='M', INCOME='427500', FAMILY_STATUS='Civil marriage', HOUSING_TYPE='Rent', FAM_MEMBERS='2', STATUS='X')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "651e6daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='5024491', GENDER='M', INCOME='135000', FAMILY_STATUS='Married', HOUSING_TYPE='Rent', FAM_MEMBERS='3', STATUS=None)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8c4d5dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'income'>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1baeee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID GENDER  INCOME   FAMILY_STATUS HOUSING_TYPE FAM_MEMBERS STATUS\n",
      "0     5008804      M  427500  Civil marriage         Rent           2      X\n",
      "1     5008805      M  427500  Civil marriage         Rent           2      0\n",
      "2     5008806      M  112500         Married        House           2      0\n",
      "3     5008808      F  270000          Single        House           1      0\n",
      "4     5008809      F  270000          Single        House           1      C\n",
      "...       ...    ...     ...             ...          ...         ...    ...\n",
      "4995  5024487      F  225000          Single        House           1      0\n",
      "4996  5024488      M  171000         Married        House           2      0\n",
      "4997  5024489      M  171000         Married        House           2      0\n",
      "4998  5024490      M  135000         Married         Rent           3      0\n",
      "4999  5024491      M  135000         Married         Rent           3   None\n",
      "\n",
      "[5000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#convert it to Python Pandas DataFrame.\n",
    "pdf = df.toPandas()\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3df4fbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a3c16e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+--------------+------------+-----------+------+\n",
      "|     ID|GENDER|INCOME| FAMILY_STATUS|HOUSING_TYPE|FAM_MEMBERS|STATUS|\n",
      "+-------+------+------+--------------+------------+-----------+------+\n",
      "|5008804|     M|427500|Civil marriage|        Rent|          2|     X|\n",
      "|5008805|     M|427500|Civil marriage|        Rent|          2|     0|\n",
      "|5008806|     M|112500|       Married|       House|          2|     0|\n",
      "|5008808|     F|270000|        Single|       House|          1|     0|\n",
      "|5008809|     F|270000|        Single|       House|          1|     C|\n",
      "|5008810|     F|270000|        Single|       House|          1|     C|\n",
      "|5008811|     F|270000|        Single|       House|          1|     C|\n",
      "|5008812|     F|283500|     Separated|       House|          1|     C|\n",
      "|5008813|     F|283500|     Separated|       House|          1|     C|\n",
      "|5008814|     F|283500|     Separated|       House|          1|     C|\n",
      "+-------+------+------+--------------+------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c3e15dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='5008804', GENDER='M', INCOME='427500', FAMILY_STATUS='Civil marriage', HOUSING_TYPE='Rent', FAM_MEMBERS='2', STATUS='X'),\n",
       " Row(ID='5008805', GENDER='M', INCOME='427500', FAMILY_STATUS='Civil marriage', HOUSING_TYPE='Rent', FAM_MEMBERS='2', STATUS='0'),\n",
       " Row(ID='5008806', GENDER='M', INCOME='112500', FAMILY_STATUS='Married', HOUSING_TYPE='House', FAM_MEMBERS='2', STATUS='0'),\n",
       " Row(ID='5008808', GENDER='F', INCOME='270000', FAMILY_STATUS='Single', HOUSING_TYPE='House', FAM_MEMBERS='1', STATUS='0'),\n",
       " Row(ID='5008809', GENDER='F', INCOME='270000', FAMILY_STATUS='Single', HOUSING_TYPE='House', FAM_MEMBERS='1', STATUS='C')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "094ad956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|     id|income|status|\n",
      "+-------+------+------+\n",
      "|5008804|427500|     X|\n",
      "|5008805|427500|     0|\n",
      "|5008806|112500|     0|\n",
      "|5008808|270000|     0|\n",
      "|5008809|270000|     C|\n",
      "|5008810|270000|     C|\n",
      "|5008811|270000|     C|\n",
      "|5008812|283500|     C|\n",
      "|5008813|283500|     C|\n",
      "|5008814|283500|     C|\n",
      "|5008815|270000|     C|\n",
      "|5112956|270000|     C|\n",
      "|6153651|270000|     C|\n",
      "|5008819|135000|     0|\n",
      "|5008820|135000|     0|\n",
      "|5008821|135000|     0|\n",
      "|5008822|135000|     0|\n",
      "|5008823|135000|     0|\n",
      "|5008824|135000|     0|\n",
      "|5008825|130500|     0|\n",
      "+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id','income','status').show() #SELECT COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c2ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26ba7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a5cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f125a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683d33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the name of the single column:\n",
    "cases = cases.withColumnRenamed(\"infection_case\",\"infection_source\")\n",
    "#for all columns:\n",
    "cases = cases.toDF(*['case_id', 'province', 'city', 'group', 'infection_case', 'confirmed','latitude', 'longitude'])\n",
    "\n",
    "\n",
    "#SORT\n",
    "cases.sort(\"confirmed\").show()\n",
    "\n",
    "# descending Sort\n",
    "from pyspark.sql import functions as F\n",
    "cases.sort(F.desc(\"confirmed\")).show()\n",
    "\n",
    "#CAST\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "cases = cases.withColumn('confirmed', F.col('confirmed').cast(IntegerType()))\n",
    "cases = cases.withColumn('city', F.col('city').cast(StringType()))\n",
    "\n",
    "\n",
    "#FILTER\n",
    "\n",
    "cases.filter((cases.confirmed>10) & (cases.province=='Daegu')).show()\n",
    "\n",
    "#GROUPBY\n",
    "from pyspark.sql import functions as F\n",
    "cases.groupBy([\"province\",\"city\"]).agg(F.sum(\"confirmed\") ,F.max(\"confirmed\")).show()\n",
    "\n",
    "#alias keyword to rename columns\n",
    "cases.groupBy([\"province\",\"city\"]).agg(F.sum(\"confirmed\").alias(\"TotalConfirmed\"),F.max(\"confirmed\").alias(\"MaxFromOneConfirmedCase\")).show()\n",
    "\n",
    "\n",
    "#JOINS\n",
    "cases = cases.join(regions, ['province','city'],how='left')\n",
    "cases.limit(10).toPandas()\n",
    "\n",
    "#Broadcast/Map Side Joins\n",
    "from pyspark.sql.functions import broadcast\n",
    "cases = cases.join(broadcast(regions), ['province','city'],how='left')\n",
    "\n",
    "\n",
    "#Use SQL With Data Frames\n",
    "cases.registerTempTable('cases_table')\n",
    "newDF = sqlContext.sql('select * from cases_table where confirmed>100')\n",
    "newDF.show()\n",
    "\n",
    "#Create New Columns\n",
    "#USING SPARK NATIVE FUNCTIONS\n",
    "import pyspark.sql.functions as F\n",
    "casesWithNewConfirmed = cases.withColumn(\"NewConfirmed\", 100 + F.col(\"confirmed\"))\n",
    "casesWithNewConfirmed.show()\n",
    "#math functions like the F.exp function:\n",
    "casesWithExpConfirmed = cases.withColumn(\"ExpConfirmed\", F.exp(\"confirmed\"))\n",
    "casesWithExpConfirmed.show()\n",
    "\n",
    "\n",
    "#USING SPARK UDFS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "def casesHighLow(confirmed):\n",
    "    if confirmed < 50: \n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "    \n",
    "#convert to a UDF Function by passing in the function and return type of function\n",
    "casesHighLowUDF = F.udf(casesHighLow, StringType())\n",
    "CasesWithHighLow = cases.withColumn(\"HighLow\", casesHighLowUDF(\"confirmed\"))\n",
    "CasesWithHighLow.show()\n",
    "\n",
    "\n",
    "#USING RDDS\n",
    "    import math\n",
    "    from pyspark.sql import Row\n",
    "    def rowwise_function(row):\n",
    "        # convert row to python dictionary:\n",
    "        row_dict = row.asDict()\n",
    "        # Add a new key in the dictionary with the new column name and \n",
    "    value.\n",
    "        # This might be a big complex function.\n",
    "        row_dict['expConfirmed'] = float(np.exp(row_dict['confirmed']))\n",
    "        # convert dict to row back again:\n",
    "        newrow = Row(**row_dict)\n",
    "        # return new row\n",
    "        return newrow\n",
    "    # convert cases dataframe to RDD\n",
    "    cases_rdd = cases.rdd\n",
    "    # apply our function to RDD\n",
    "    cases_rdd_new = cases_rdd.map(lambda row: rowwise_function(row))\n",
    "    # Convert RDD Back to DataFrame\n",
    "    casesNewDf = sqlContext.createDataFrame(cases_rdd_new)\n",
    "    casesNewDf.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "#USING PANDAS UDF\n",
    "cases.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbb609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, BooleanType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "# Declare the schema for the output of our function\n",
    "\n",
    "outSchema = StructType([StructField('case_id',IntegerType(),True),\n",
    "                        StructField('province',StringType(),True),\n",
    "                        StructField('city',StringType(),True),\n",
    "                        StructField('group',BooleanType(),True),\n",
    "                        StructField('infection_case',StringType(),True),\n",
    "                        StructField('confirmed',IntegerType(),True),\n",
    "                        StructField('latitude',StringType(),True),\n",
    "                        StructField('longitude',StringType(),True),\n",
    "                        StructField('normalized_confirmed',DoubleType(),True)\n",
    "                       ])\n",
    "# decorate our function with pandas_udf decorator\n",
    "@F.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    v = pdf.confirmed\n",
    "    v = v - v.mean()\n",
    "    pdf['normalized_confirmed'] = v\n",
    "    return pdf\n",
    "\n",
    "confirmed_groupwise_normalization = cases.groupby(\"infection_case\").apply(subtract_mean)\n",
    "\n",
    "confirmed_groupwise_normalization.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Window Functions\n",
    "\n",
    "#RANKING'\n",
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window().partitionBy(['province']).orderBy(F.desc('confirmed'))\n",
    "cases.withColumn(\"rank\",F.rank().over(windowSpec)).show()\n",
    "\n",
    "\n",
    "#LAG VARIABLES\n",
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window().partitionBy(['province']).orderBy('date')\n",
    "timeprovinceWithLag = timeprovince.withColumn(\"lag_7\",F.lag(\"confirmed\", 7).over(windowSpec))\n",
    "timeprovinceWithLag.filter(timeprovinceWithLag.date>'2020-03-10').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a08f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROLLING AGGREGATIONS\n",
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window().partitionBy(['province']).orderBy('date').rowsBetween(-6,0)\n",
    "timeprovinceWithRoll = timeprovince.withColumn(\"roll_7_confirmed\",F.mean(\"confirmed\").over(windowSpec))\n",
    "timeprovinceWithRoll.filter(timeprovinceWithLag.date>'2020-03-10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window().partitionBy(['province']).orderBy('date').rowsBetween(Window.\n",
    "unboundedPreceding,Window.currentRow)\n",
    "timeprovinceWithRoll = timeprovince.withColumn(\"cumulative_confirmed\",F.sum(\"confirmed\").over\n",
    "(windowSpec))\n",
    "timeprovinceWithRoll.filter(timeprovinceWithLag.date>'2020-03-\n",
    "10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot Data Frames\n",
    "pivotedTimeprovince = \n",
    "timeprovince.groupBy('date').pivot('province').agg(F.sum('confirmed').\n",
    "alias('confirmed') , F.sum('released').alias('released'))\n",
    "pivotedTimeprovince.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unpivot/Stack Data Frames\n",
    "\n",
    "newColnames = [x.replace(\"-\",\"_\") for x in \n",
    "pivotedTimeprovince.columns]\n",
    "pivotedTimeprovince = pivotedTimeprovince.toDF(*newColnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivotedTimeprovince = pivotedTimeprovince.select('date',F.expr(exprs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salting\n",
    "\n",
    "#STEP ONE: CREATE A SALTING KEY\n",
    "cases = cases.withColumn(\"salt_key\", F.concat(F.col(\"infection_case\"), F.lit(\"_\"), F.monotonically_increasing_id() % 10))\n",
    "#STEP TWO: FIRST GROUPBY ON SALT KEY\n",
    "cases_temp = cases.groupBy([\"infection_case\",\"salt_key\"]).agg(F.sum(\"confirmed\")).alias(\"salt_confirmed\").show()\n",
    "\n",
    "#3. SECOND GROUP ON THE ORIGINAL KEY\n",
    "cases_temp = cases.groupBy([\"infection_case\"]).agg(F.sum(\"confirmed\")).alias(\"salt_confirmed\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CACHING\n",
    "df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE AND LOAD FROM AN INTERMEDIATE STEP\n",
    "df.write.parquet(\"data/df.parquet\")\n",
    "df.unpersist()\n",
    "spark.read.load(\"data/df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPARTITIONING\n",
    "df = df.repartition(1000)\n",
    "df = df.repartition('cola', 'colb','colc','cold')\n",
    "df.rdd.getNumPartitions()\n",
    "df.glom().map(len).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a14cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING PARQUET FILE IN LOCAL\n",
    "\n",
    "from glob import glob\n",
    "def load_df_from_parquet(parquet_directory):\n",
    "   df = pd.DataFrame()\n",
    "   for file in glob(f\"{parquet_directory}/*\"):\n",
    "      df = pd.concat([df,pd.read_parquet(file)])\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession Commonly Used Methods\n",
    "\n",
    "# version() – Returns the Spark version where your application is running, probably the Spark version your cluster is configured with.\n",
    "\n",
    "# createDataFrame() – This creates a DataFrame from a collection and an RDD\n",
    "\n",
    "# getActiveSession() – \n",
    "\n",
    "# read() – Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro, and more file formats into DataFrame.\n",
    "\n",
    "# readStream() – Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to read streaming data into DataFrame.\n",
    "\n",
    "# sparkContext() – Returns a SparkContext.\n",
    "\n",
    "# sql() – Returns a DataFrame after executing the SQL mentioned.\n",
    "\n",
    "# sqlContext() – Returns SQLContext.\n",
    "\n",
    "# stop() – Stop the current SparkContext.\n",
    "\n",
    "# table() – Returns a DataFrame of a table or view.\n",
    "\n",
    "# udf() – Creates a PySpark UDF to use it on DataFrame, Dataset, and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark DataFrame\n",
    "# SparkSession also provides several methods to create a Spark DataFrame and DataSet. The below example uses the createDataFrame() method which takes a list of data.\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804173c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.3 Working with Spark SQL\n",
    "Using SparkSession you can access PySpark/Spark SQL capabilities in PySpark. In order to use SQL features first, you need to create a temporary view in PySpark. Once you have a temporary view you can run any ANSI SQL queries using spark.sql() method.\n",
    "\n",
    "\n",
    "# Spark SQL\n",
    "df.createOrReplaceTempView(\"sample_table\")\n",
    "df2 = spark.sql(\"SELECT _1,_2 FROM sample_table\")\n",
    "df2.show()\n",
    "\n",
    "\n",
    "PySpark SQL temporary views are session-scoped and will not be available if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view using createGlobalTempView()\n",
    "\n",
    "4.4 Create Hive Table\n",
    "As explained above SparkSession is used to create and query Hive tables. Note that in order to do this for testing you don’t need Hive to be installed. saveAsTable() creates Hive managed table. Query the table using spark.sql().\n",
    "\n",
    "\n",
    "# Create Hive table & query it.  \n",
    "spark.table(\"sample_table\").write.saveAsTable(\"sample_hive_table\")\n",
    "df3 = spark.sql(\"SELECT _1,_2 FROM sample_hive_table\")\n",
    "df3.show()\n",
    "\n",
    "\n",
    "4.5 Working with Catalogs\n",
    "To get the catalog metadata, PySpark Session exposes catalog variable. Note that these methods spark.catalog.listDatabases and spark.catalog.listTables and returns the DataSet.\n",
    "\n",
    "\n",
    "# Get metadata from the Catalog\n",
    "# List databases\n",
    "dbs = spark.catalog.listDatabases()\n",
    "print(dbs)\n",
    "\n",
    "# Output\n",
    "#[Database(name='default', description='default database', \n",
    "#locationUri='file:/Users/admin/.spyder-py3/spark-warehouse')]\n",
    "\n",
    "# List Tables\n",
    "tbls = spark.catalog.listTables()\n",
    "print(tbls)\n",
    "\n",
    "#Output\n",
    "#[Table(name='sample_hive_table', database='default', description=None, tableType='MANAGED', #isTemporary=False), Table(name='sample_hive_table1', database='default', description=None, #tableType='MANAGED', isTemporary=False), Table(name='sample_hive_table121', database='default', #description=None, tableType='MANAGED', isTemporary=False), Table(name='sample_table', database=None, #description=None, tableType='TEMPORARY', isTemporary=True)]\n",
    "\n",
    "Notice the two tables we have created, Spark table is considered a temporary table and Hive table as managed table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
