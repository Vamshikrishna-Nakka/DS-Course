{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ff92e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4192693043.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    conda install -c cyclus java-jdk\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark\n",
    "# conda install -c cyclus java-jdk\n",
    "\n",
    "# pip install findspark\n",
    "# !pip install pyspark\n",
    "# conda install -c conda-forge py4j\n",
    "# conda install -c conda-forge/label/gcc7 py4j\n",
    "# conda install -c conda-forge/label/cf201901 py4j\n",
    "# conda install -c conda-forge/label/cf202003 py4j\n",
    "\n",
    "# import os\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"/anaconda3/envs/DIRECTORYNAME/bin/python\"\n",
    "\n",
    "\n",
    "# SPARK_HOME    C:\\Users\\vamshikrishna.n\\Documents\\Spark\\spark-3.3.0-bin-hadoop3\n",
    "# HADOOP_HOME   C:\\Users\\vamshikrishna.n\\Documents\\Spark\\spark-3.3.0-bin-hadoop3\n",
    "# PYSPARK_DRIVER_PYTHON   jupyter\n",
    "# PYSPARK_DRIVER_PYTHON_OPTS   notebook\n",
    "\n",
    "# %JAVA_HOME%\\bin\n",
    "# %HADOOP_HOME%\\bin\n",
    "# %SPARK_HOME%\\bin\n",
    "\n",
    "#  http://localhost:4040/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc2bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Spark in Python  ----- Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "439e6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "\n",
    "# from pyspark.conf import SparkConf\n",
    "# from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7a11cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets \n",
    "# initiated here.pyspark.SparkContext is an entry point to the PySpark functionality that is used to communicate with the \n",
    "# cluster and to create an RDD, accumulator, and broadcast variables. but we can create only one SparkContext per JVM, in \n",
    "# order to create another first you need to stop the existing one using stop() method.\n",
    "\n",
    "# Spark driver program creates and uses SparkContext to connect to the cluster manager to submit PySpark jobs,and know what\n",
    "# resource manager (YARN, Mesos, or Standalone) to communicate to. It is the heart of the PySpark application.\n",
    "\n",
    "# SparkContext uses Py4J to launch a JVM and creates a JavaSparkContext. By default, PySpark has SparkContext shell creates\n",
    "# and provides ‘sc’ object, which is an instance of SparkContext class. so creating a new SparkContext won't work. We can \n",
    "# directly use this object where required without the need of creating.\n",
    "\n",
    "# At any given time only one SparkContext instance should be active per JVM. In case you want to create another you should \n",
    "# stop existing SparkContext using stop() before creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b04d434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn=SparkConf().setAppName('Test').setMaster('local')   # cn = SparkConf(loadDefaults=False)\n",
    "# Once a SparkConf object is passed to Spark, it is cloned and can no longer be modified by the user.\n",
    "\n",
    "# sparkconf.get(\"spark.master\")    # to know them before assigning them to Entrypoint.\n",
    "# sparkconf.get(\"spark.app.name\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b2c7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext(conf=cn)    # You can also create it using SparkContext.getOrCreate()\n",
    "# sc = SparkContext.getOrCreate(cn)  #It actually returns an existing active SparkContext otherwise creates one with a specified master and app name.\n",
    "# sc = SparkContext(\"local\", \"Spark_Example_App\") # master as local and app name as Spark_Example_App.\n",
    "\n",
    "\n",
    "\n",
    "# To create a SparkContext you first need to build a SparkConf object that contains information about your application.\n",
    "# Test is the appname to show on the cluster UI\n",
    "# local is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7cb0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "894293f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.master  \n",
    "# sc.version – Version of PySpark cluster where your job is running.\n",
    "# sc.appName \n",
    "# sc.applicationId – Returns a unique ID of a PySpark application.\n",
    "# sc.uiWebUrl – Provides the Spark Web UI url that started by SparkContext.\n",
    "# sc.sparkHome is None   # cn.setSparkHome(\"/path\")\n",
    "\n",
    "#sc.stop() # When executes this, it logs the message INFO SparkContext: Successfully stopped SparkContext to console or to a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd28cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "\n",
    "# contains(key)-- Does this configuration contain a given key?\n",
    "# get(key[, defaultValue]) -- Get the configured value for some key, or return a default otherwise.\n",
    "  # conf.get(\"spark.executorEnv.VAR1\")\n",
    "\n",
    "# getAll()--  Get all values as a list of key-value pairs. \n",
    "    # for p in sorted(conf.getAll(), key=lambda p: p[0]):\n",
    "    #     print(p)\n",
    "    \n",
    "# set(key, value) -- Set a configuration property.\n",
    "# setAll(pairs)-- Set multiple parameters, passed as a list of key-value pairs.\n",
    "# setAppName(value)-- Set application name.\n",
    "# setExecutorEnv([key, value, pairs]) -- Set an environment variable to be passed to executors.\n",
    "    # conf.setExecutorEnv(\"VAR1\", \"value1\")\n",
    "    # conf.setExecutorEnv(pairs = [(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
    "    # conf._jconf.setExecutorEnv(\"VAR5\", \"value5\")\n",
    "    \n",
    "# setIfMissing(key, value)--  Set a configuration property, if not already set.\n",
    "# setMaster(value)-- Set master URL to connect to.\n",
    "# setSparkHome(value)--  Set path where Spark is installed on worker nodes.\n",
    "# toDebugString() -- Returns a printable version of the configuration, as a list of key=value pairs, one per line.\n",
    "    # print(conf.toDebugString())\n",
    "\n",
    "# sc.getConf() -- gives all configurations\n",
    "# sc.cancelAllJobs() -Cancel all jobs that have been scheduled or are running.\n",
    "# cancelJob(int jobId) or cancelJob(int jobId, String reason) --Cancel a given job if it's scheduled or running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d0eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3880d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext Commonly Used Methods\n",
    "\n",
    "# accumulator(value[, accum_param]) – It creates an pyspark accumulator variable with initial specified value. Only a driver can access accumulator variables.\n",
    "# broadcast(value) – read-only PySpark broadcast variable. This will be broadcast to the entire cluster. You can broadcast a variable to a PySpark cluster only once.\n",
    "# emptyRDD() – Creates an empty RDD\n",
    "# getOrCreate() – Creates or returns a SparkContext\n",
    "# hadoopFile() – Returns an RDD of a Hadoop file\n",
    "# newAPIHadoopFile() – Creates an RDD for a Hadoop file with a new API InputFormat.\n",
    "# sequenceFile() – Get an RDD for a Hadoop SequenceFile with given key and value types.\n",
    "# setLogLevel() – Change log level to debug, info, warn, fatal, and error\n",
    "# textFile() – Reads a text file from HDFS, local or any Hadoop supported file systems and returns an RDD\n",
    "# union() – Union two RDDs\n",
    "# wholeTextFiles() – Reads a text file in the folder from HDFS, local or any Hadoop supported file systems and returns an RDD of Tuple2. The first element of the tuple consists file name and the second element consists context of the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77560a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the use of getOrCreate() method in SparkContext Class?\n",
    "\n",
    "# You can share a SparkContext only within a single JVM (The JVM in which the Spark Driver is running). \n",
    "# So, sharing of SparkContext only happens between applications running within the same Driver JVM. Within the same JVM, \n",
    "# getOrCreate() will give you the same instance of SparkContext; and this will help you share broadcast variables, etc \n",
    "# among different applications spawned by the same Spark Driver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82e1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2e17721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3167401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession was introduced in version 2.0, It is an entry point to underlying PySpark functionality in order to \n",
    "# programmatically create PySpark RDD, DataFrame. It’s object spark is default available in pyspark-shell and it can be \n",
    "# created programmatically using SparkSession. Prior to 2.0,SparkContext used to be an entry point.\n",
    "\n",
    "# With Spark 2.0 a new class \"SparkSession (pyspark.sql import SparkSession)\" has been introduced. SparkSession is a combined \n",
    "# class for all different contexts we used to have prior to 2.0 release (SQLContext and HiveContext e.t.c). Since 2.0 \n",
    "# SparkSession can be used in replace with SQLContext, HiveContext, and other contexts defined prior to 2.0.\n",
    "\n",
    "# SparkSession will be created using 'SparkSession.builder' builder patterns. SparkContext is not completely replaced with SparkSession,\n",
    "# many features of SparkContext are still available and used in Spark 2.0 and later. You should also know that SparkSession \n",
    "# internally creates SparkConfig and SparkContext with the configuration provided with SparkSession.\n",
    "\n",
    "\n",
    "# SparkSession also includes all the APIs available in different contexts –\n",
    "#     SparkContext,\n",
    "#     SQLContext,\n",
    "#     StreamingContext,\n",
    "#     HiveContext\n",
    " \n",
    "# we can create as many SparkSession as you want in a PySpark application using either SparkSession.builder() or \n",
    "# SparkSession.newSession(). Many Spark session objects are required when you wanted to keep PySpark tables (relational \n",
    "# entities) logically separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f25fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession from builder\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b0bf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Catalog',\n",
       " 'Column',\n",
       " 'DataFrame',\n",
       " 'DataFrameNaFunctions',\n",
       " 'DataFrameReader',\n",
       " 'DataFrameStatFunctions',\n",
       " 'DataFrameWriter',\n",
       " 'GroupedData',\n",
       " 'HiveContext',\n",
       " 'Observation',\n",
       " 'PandasCogroupedOps',\n",
       " 'Row',\n",
       " 'SQLContext',\n",
       " 'SparkSession',\n",
       " 'UDFRegistration',\n",
       " 'Window',\n",
       " 'WindowSpec',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'catalog',\n",
       " 'column',\n",
       " 'conf',\n",
       " 'context',\n",
       " 'dataframe',\n",
       " 'functions',\n",
       " 'group',\n",
       " 'observation',\n",
       " 'pandas',\n",
       " 'readwriter',\n",
       " 'session',\n",
       " 'sql_formatter',\n",
       " 'streaming',\n",
       " 'types',\n",
       " 'udf',\n",
       " 'utils',\n",
       " 'window']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark.sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427a275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "\n",
    "# master() – If you are running it on the cluster you need to use your master name as an argument to master(). usually, \n",
    "#             it would be either \"yarn\" or \"mesos\" depends on your cluster setup.\n",
    "\n",
    "#local[x] -- when running in Standalone mode. x should be an integer value and should be greater than 0; this represents \n",
    "#             how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the \n",
    "#             number of CPU cores you have.\n",
    "\n",
    "# appName() – Used to set your application name.\n",
    "\n",
    "# getOrCreate() – This returns a SparkSession object if already exists, and creates a new one if not exist.\n",
    "\n",
    "# Note:  SparkSession object spark is by default available in the PySpark shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2de89ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01880fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Another SparkSession\n",
    "\n",
    "# we can also create a new SparkSession using newSession() method. This uses the same app name, master as the existing \n",
    "# session. Underlying SparkContext will be the same for both sessions as you can have only one context per PySpark \n",
    "# application.\n",
    "\n",
    "spark2 = SparkSession.newSession  #This always creates a new SparkSession object.\n",
    "print(spark2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Existing SparkSession\n",
    "\n",
    "# we can get the existing SparkSession in PySpark using the builder.getOrCreate()\n",
    "\n",
    "spark3 = SparkSession.builder.getOrCreate\n",
    "print(spark3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spark Config\n",
    "# If we wanted to set some configs to SparkSession, use the config() method.\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\") \\\n",
    "      .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab64901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef2859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession with Hive Enable\n",
    "# In order to use Hive with PySpark, you need to enable it using the enableHiveSupport() method.\n",
    "\n",
    "\n",
    "# Enabling Hive to use in Spark\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .config(\"spark.sql.warehouse.dir\", \"<path>/spark-warehouse\") \\\n",
    "      .enableHiveSupport() \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PySpark Configs\n",
    "# Once the SparkSession is created, you can add the spark configs during runtime or get all configs.\n",
    "\n",
    "\n",
    "# Set Config\n",
    "spark.conf.set(\"spark.executor.memory\", \"5g\")\n",
    "\n",
    "# Get a Spark Config\n",
    "partions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(partions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44356131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a87dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version # version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77eac45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LT387.CORP.SYSTECHMNG.NET:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x257bdfa8a90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.getActiveSession()#returns an active Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf619a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
